constants {
  default-load = 1.0
  retry-timeout = 4 // s time to wait before re-sending a msg
  default-retries = 5 // number of attempts to send a message (can be used for TCEPUtils.trySendWithReply()
  default-request-timeout = 15 // s, used for load/bandwidth/etc requests
  coordinates-refresh-interval = 5
  coordinate-request-timeout = 15
  coordinates-error-threshold = 4.0
  transition-execution-mode = 1 # Can be 0 for "Sequential" or 1 for "Concurrent"
  event-interval-microseconds = 1000000 # ~1 events/second
  default-data-rate = 30.0 # Mbit/s Default data rate to use when there is no measurement yet
  data-rate-refresh-interval = 240
  simulation-time = 2
  gui-endpoint = "http://gui:3000"
  host-ip-addresses = ["171.67.2.43", "171.67.2.48", "171.67.2.46", "171.67.2.49", "171.67.2.50", "171.67.2.44", "171.67.2.47"]
  number-of-speed-publisher-nodes = 3 # number of publisher nodes that are SpeedPublishers
  number-of-road-sections = 3 # must be even; number of sections the road is divided into, i.e. points at which mobility is simulated by switching connections of publisher containers; affects "granularity" of mobility simulation
  mininet-simulation = false
  mininet-link-bandwidth = 100.0 // Mbit/s; link bandwidth set in mininet simulation
  global-initialization-timeout = 60000 // ms
  base-port = 2500

  mobility-simulation {
    //enabled = true
    //delay = 5 // s, delay before publishers start sending events in a mobility simulation (to avoid running out of mobility trace data since only ~25 minutes available; simulation start is delayed by bandwidth measurements)
    road-section-length = 2500
  }
  // the MAPEK-Implementation controlling transitions to use
 // requirementBased: make transition based on QoS requirements (see benchmark configuration section)
  //    - triggers for execution: QoS requirement changes, nodes being shut down or started
  mapek {
    type = "requirementBased" // values: "requirementBased", "lightweight"
    availableTypes = ["requirementBased", "lightweight"]
    sampling-interval = 1 // s
    blacklisted-algorithms = ["fsRandom"] // algorithms that will never be selected by CONTRAST
    enable-distributed-transition-debugging = true // enable (pub/sub) debug logging for transitions (causes additional messaging overhead)
    sampling-interval = 1 // seconds how often the learning model receives data
    transition-cooldown = 30 // s; time that must have passed before allowing another transition -> avoid oscillating transitions
    improvement-threshold = 0.0 // percentage of predicted metric improvement over current value that is necessary to allow a transition -> take predicition inaccuracy into account when making transition decision
    only-suitable = false // use only OP mechanisms, which optimize for the given QoS requirements. Not necessary anymore
    lightweight-decay = 0.5 // Weighting for old vs new value. [0 , 0.5]
  }

  placement {
    placement-request-timeout = 30 // s
    physical-placement-node-overload-threshold = 3.0 // unix cpu load level at which a node is considered overloaded
    physical-placement-nearest-neighbours = 3
    relaxation-initial-step-size = 0.1
    relaxation-step-adjustment-enabled = true // does not work well (overflows, oscillations) without stepsize adjustment
    max-single-operator-iterations = 200
    relaxation-initial-iterations = 30
    update-interval = 60 // seconds after which node re-evaluates its own operators placement
    update { // enable or disable placement update functionality
      relaxation = false
      rizou = false
      starks = false
    }
  }

  query = ["Stream", "Filter", "Disjunction", "Join", "SelfJoin"]
  //query = ["Stream", "Filter", "Conjunction", "Disjunction", "Join", "SelfJoin", "Accident Detection"]
  //query = ["Conjunction"]

}

benchmark {
  general {
    algorithms = [
      "Relaxation",
      "Rizou",
      "ProducerConsumer",
      "Random",
      "GlobalOptimalBDP",
      "MDCEP"
    ]
  }

  Relaxation {
    optimizationCriteria = ["latency", "machineLoad"]
    constraints = ["LowChurnRate"]
    class = "tcep.placement.sbon.PietzuchAlgorithm$"
    score = 100
  }

  MDCEP {
    optimizationCriteria = ["messageHops", "machineLoad"]
    constraints = ["LowChurnRate"]
    class = "tcep.placement.manets.StarksAlgorithm$"
    score = 100
  }

  Rizou {
    optimizationCriteria = ["latency", "machineLoad"] # does optimize for latency as well, but Relaxation has the same criteria -> make both selectable in RequirementBasedMAPEK via requirement change
    constraints = ["LowChurnRate"]
    class = "tcep.placement.mop.RizouAlgorithm$"
    score = 100
  }

  ProducerConsumer {
    optimizationCriteria = ["messageHops"]
    constraints = ["HighChurnRate"]
    class = "tcep.placement.MobilityTolerantAlgorithm$"
    score = 100
  }

  Random {
    optimizationCriteria = []
    constraints = ["LowChurnRate"]
    class = "tcep.placement.RandomAlgorithm$"
    score = 200
  }

  GlobalOptimalBDP {
    optimizationCriteria = ["latency", "messageHops"]
    constraints = ["LowChurnRate"]
    class = "tcep.placement.GlobalOptimalBDPAlgorithm$"
    score = 100
  }
}

# define prio mailbox to be passed in Props of new actors that handle messages of differing priority
prio-mailbox {
  mailbox-type = "tcep.akkamailbox.TCEPPriorityMailbox"
}

# dedicated dispatcher to be used by all disk- or network I/O-bound tasks and futures
# this alias points to the default blocking-io-dispatcher, which is shared for some akka-related tasks. Use and configure custom dispatcher below instead if system behaves erratically under load due to thread starvation
#blocking-io-dispatcher = "akka.actor.default-blocking-io-dispatcher"
# TODO check if this works out
blocking-io-dispatcher {
  type = Dispatcher
# might use forkjoin-executor instead
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-min = 4
     parallelism-factor = 1.0
     # Note that the parallelism-max does not set the upper bound on the total number of threads allocated by the ForkJoinPool.
     # It is a setting specifically talking about the number of hot threads the pool keep running in order to reduce the latency of handling a new incoming task
     parallelism-max = 12
  }
  # Throughput defines the maximum number of messages to be
  # processed per actor before the thread jumps to the next actor.
  # Set to 1 for as fair as possible.
  throughput = 1
}

akka {
  # Loggers to register at boot time (akka.event.Logging$DefaultLogger logs
  # to STDOUT)
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logger-startup-timeout = 30s
  jvm-exit-on-fatal-error = false

  # Log level used by the configured loggers (see "loggers") as soon
  # as they have been started; before that, see "stdout-loglevel"
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  loglevel = "DEBUG"

  # Log level for the very basic logger activated during ActorSystem startup.
  # This logger prints the log messages to stdout (System.out).
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  stdout-loglevel = "DEBUG"

  # Filter of log events that is usved by the LoggingAdapter before
  # publishing log events to the eventStream.
  # logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

  log-config-on-start = off
  log-dead-letters = off
  log-dead-letters-during-shutdown = on

  actor {
    debug.receive = off
    debug.unhandled = on

    provider = "cluster"
    timeout = 60000

    allow-java-serializer-usage = false // disable java serialization to ensure kryo is always used
    warn-about-java-serializer-usage = true
    serializers {
      java = "akka.serialization.JavaSerializer"
      # Define kryo serializer
      kryo = "com.twitter.chill.akka.AkkaSerializer"
    }

    # assign a mailbox type (prio-mailbox) to all actors with the name "TaskManager" and "DistVivaldiRef", even without specifying the mailbox in the Props
    deployment {
      /TaskManager {
        mailbox = prio-mailbox
      }
      /DistVivaldiRef {
        mailbox = prio-mailbox
      }
      /ClientNode {
        mailbox = prio-mailbox
      }
      /knowledge {
        mailbox = prio-mailbox
      }
    }

    serialization-bindings {

      "java.io.Serializable" = kryo
      #"tcep.data.Events$" = kryo
      "tcep.data.Events$Event" = kryo
      "tcep.machinenodes.helper.actors.Message" = kryo
    }
  }

  remote {
    classic { # classic remoting with netty.tcp, disabled if artery is enabled
        # If this is "on", Akka will log all outbound messages at DEBUG level,
        # if off then they are not logged
        log-sent-messages = on
        log-received-messages = on
        log-frame-size-exceeding = 1b
        log-remote-lifecycle-events = debug

        transport-failure-detector {
              # FQCN of the failure detector implementation.
              # It must implement akka.remote.FailureDetector and have
              # a public constructor with a com.typesafe.config.Config and
              # akka.actor.EventStream parameter.
              implementation-class = "akka.remote.DeadlineFailureDetector"
              # How often keep-alive heartbeat messages should be sent to each connection.
              heartbeat-interval = 1 s
              # Number of potentially lost/delayed heartbeats that will be
              # accepted before considering it to be an anomaly.
              # A margin to the `heartbeat-interval` is important to be able to survive sudden,
              # occasional, pauses in heartbeat arrivals, due to for example garbage collect or
              # network drop.
              acceptable-heartbeat-pause = 5 s # default 120 s
         }

         # After failed to establish an outbound connection, the remoting will mark the
         # address as failed. This configuration option controls how much time should
         # be elapsed before reattempting a new connection. While the address is
         # gated, all messages sent to the address are delivered to dead-letters.
         # Since this setting limits the rate of reconnects setting it to a
         # very short interval (i.e. less than a second) may result in a storm of
         # reconnect attempts.
         retry-gate-closed-for = 500 ms # default 5s

         netty.tcp {
          bind-hostname = "0.0.0.0"
          message-frame-size = 30000000b
          send-buffer-size = 30000000b
          receive-buffer-size = 30000000b
          maximum-frame-size = 30000000b
          connection-timeout = 10 s # default 15s; how long a connect may take until it is timed out

          # LEAVE THIS OFF FOR MININET SIMULATIONS, otherwise heartbeats get lost randomly when handover (publisher movement) happens
          # -> members become marked unreachable (despite being pingable)
          # -> connections to members get gated and all messages to them are sent to deadLetters
          # -> transition messages cannot be delivered until the whole cluster has marked the member as reachable again (messages for this are subject to the same issue!)
          # -> seemingly random failures, chaos ensues
          # Enables TCP Keepalive, subject to the O/S kernelâ€™s configuration
          tcp-keepalive = off # default on;
        }
    }
    artery { # default since akka 2.6.0; uses separate dispatcher for control stream to avoid thread starvation
          # Enable the new remoting with this flag, off is netty.tcp
          enabled = off
          log-received-messages = on
          log-sent-messages = on
          # Hostname to bind a network interface to. Can be set to an ip, hostname
          # or one of the following special values:
          #   "0.0.0.0"            all interfaces
          #   ""                   akka.remote.artery.canonical.hostname
          #   "<getHostAddress>"   InetAddress.getLocalHost.getHostAddress
          #   "<getHostName>"      InetAddress.getLocalHost.getHostName
          bind.hostname = "0.0.0.0"
          advanced.compression.actor-refs.advertisement-interval = 30m # default 1m, set this longer than simulation to avoid countless useless lines when debugging
     }
  }

  cluster {
    seed-nodes = [
        #Names will be resolved by Docker Network. See publish_docker.sh for more details.
        #"akka://tcep@10.0.0.253:"${?constants.base-port}""
        #"akka.tcp://tcep@10.0.0.253:"${?constants.base-port}""
        #"akka.tcp://tcep@20.0.0.15:"${?constants.base-port}""
        "akka.tcp://tcep@simulator:"${?constants.base-port}""
    ]
    retry-unsuccessful-join-after = 5s
    min-nr-of-members = 6 // set by manifest_to_config.py

    # Enable/disable info level logging of cluster events.
    # These are logged with logger name `akka.cluster.Cluster`.
    log-info = on
    # Enable/disable verbose info-level logging of cluster events
    # for temporary troubleshooting. Defaults to 'off'.
    # These are logged with logger name `akka.cluster.Cluster`.
    log-info-verbose = on
    debug {
        # log heartbeat events (very verbose, useful mostly when debugging heartbeating issues)
        verbose-heartbeat-logging = on
        # log verbose details about gossip
        verbose-gossip-logging = on
    }
    # how often should the node send out gossip information?
    gossip-interval = 1 s # default 1s
    # Gossip to random node with newer or older state information, if any with
    # this probability. Otherwise Gossip to any random live node.
    # Probability value is between 0.0 and 1.0. 0.0 means never, 1.0 means always.
    gossip-different-view-probability = 1.0 # default 0.8 -> faster convergence
    # how often should the node move nodes, marked as unreachable by the failure
    # detector, out of the membership ring?
    unreachable-nodes-reaper-interval = 5 s # default 1s
    failure-detector {
      threshold = 12.0 # default 8; higher value -> fewer false positives, longer time until actual failure detection
      acceptable-heartbeat-pause = 60 s # default 10s; leave this high so that mininet handovers do not trigger failure detector (gates connections to "unreachable" (ping on mininet says otherwise) actor until all cluster nodes have marked it as reachable again!)
      heartbeat-interval = 1 s # default 1s
      min-std-deviation = 500 ms # default 100ms
      expected-response-after = 1 s
      # Number of member nodes that each member will send heartbeat messages to,
      # i.e. each node will be monitored by this number of other nodes.
      monitored-by-nr-of-members = 3 # default 5
    }
  }
}

clustering {
  cluster.name = tcep
}

# Disable legacy metrics in akka-cluster.
akka.cluster.metrics.enabled = off

# Enable metrics extension in akka-cluster-metrics.
#akka.extensions = ["akka.cluster.metrics.ClusterMetricsExtension"]

# Sigar native library extract location during tests.
# Note: use per-jvm-instance folder when running multiple jvm on one host.
akka.cluster.metrics.native-library-extract-folder = ${user.dir}/target/native